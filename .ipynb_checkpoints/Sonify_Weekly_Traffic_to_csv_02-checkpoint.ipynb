{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01c2a8f-9904-4603-b1f6-7fef6b4b5a57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Introduction: Can You Hear the Internet?\n",
    "\n",
    "Have you ever wondered what your internet usage sounds like? We're used to seeing data in charts and graphs, but what if we could represent it with music? This was the question that sparked the Network Communications Project.\n",
    "\n",
    "In this article, I'll walk you through how I took a raw dataset of my university's weekly network traffic and transformed it into a musical piece. The goal was to \"sonify\" the download and upload statistics, creating a unique auditory representation of data.\n",
    "\n",
    "We'll use Python's Pandas library for the data manipulation and a fascinating online tool called [TwoTone MIDI Out Beta](https://twotone-midiout-beta.netlify.app/) to generate the final musical output. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189df8ed-2920-4b71-8c68-ce7963b968e5",
   "metadata": {},
   "source": [
    "# 2. The Raw Material: Sourcing the Network Data\n",
    "\n",
    "Every data story starts with the data. For this project, the source was the Janet Netsight Portal, which tracks network traffic for UK educational institutions.\n",
    "\n",
    "**The Goal:** To get a historical view of the \"In\" (download) and \"Out\" (upload) traffic for my University.  \n",
    "**The Challenge:** The portal doesn't allow for a simple \"download all\" button. Through experimentation, I found that requesting a date range of approximately 549 days (e.g., from July 2023 to January 2025) provided the daily data I needed.  \n",
    "**A Quick Note on Perspective:** The data is labeled from the provider's (Janet's) perspective. This means \"In\" is data coming into their network from the university (our upload), and \"Out\" is data going out to the university (our download). We'll need to remember to swap these later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df5ad61-27b9-4cb4-8bfe-205140044b9c",
   "metadata": {},
   "source": [
    "# 3. The Setup: Loading and Cleaning the Data  \n",
    "\n",
    "With several CSV files downloaded, the first step is to load them into a single, clean DataFrame using Pandas. We'll use Python's glob library to find all the CSV files in our input directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5019f1-db0d-4f14-a0e3-71c46b938ce0",
   "metadata": {},
   "source": [
    "## Code Block 1: Imports and File Loading  \n",
    "First, let's import our libraries and set up the path to our data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88fe1064-531e-4188-8d13-bcd51c0b21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the path to the input folder and get a list of all CSV files\n",
    "path = rf'./input/*.csv'\n",
    "files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd33eb-e2bc-4b4a-85a2-98b4ed867be0",
   "metadata": {},
   "source": [
    "Now, we'll loop through each file, read it into a Pandas DataFrame, and combine them. A small trick here is to use a header_flag to ensure we only include the header row from the very first file, creating a clean, unified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c9bac1-f210-4382-ba80-c9cf15ddc11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and filtered.\n"
     ]
    }
   ],
   "source": [
    "# List to hold each DataFrame\n",
    "df_list = []\n",
    "header_flag = False\n",
    "\n",
    "for file in files:\n",
    "    df_temp = pd.read_csv(file)\n",
    "\n",
    "    if not header_flag:\n",
    "        header_flag = True\n",
    "    else:\n",
    "        df_temp = df_temp.iloc[1:]\n",
    "\n",
    "    # --- THIS IS THE ADDED LINE ---\n",
    "    # Filter out rows where the second column (original 'Traffic Out') is zero\n",
    "    if df_temp.shape[1] > 2: # Ensure the column exists before filtering\n",
    "        df_temp = df_temp[df_temp.iloc[:, 2] != 0]\n",
    "\n",
    "    df_list.append(df_temp)\n",
    "\n",
    "# Concatenate all DataFrames into a single one\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Sort by time and rename columns to reflect our perspective\n",
    "df = df.sort_values('Time', ascending=True).copy()\n",
    "df.columns = ['Time', 'in', 'out']\n",
    "\n",
    "print(\"Data loaded and filtered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ab79d-c733-4c54-a412-7e95be56ca00",
   "metadata": {},
   "source": [
    "# 4: Feature Engineering - Building a Richer Musical Palette\n",
    "\n",
    "Raw data is just the beginning. To transform our data into a piece with rhythm, texture, and distinct movements, we need to create more features. Think of these new data columns as potential instruments or triggers in our final musical score.\n",
    "\n",
    "The goal is to create flags that mark specific points in time, such as the start of a week, a month, or a quarter. We can also add context, like whether a given day is a workday or a weekend.\n",
    "\n",
    "Here are the features we'll add to our daily data:\n",
    "\n",
    "* Week_Start: A flag for Monday to mark the start of a new week.\n",
    "* Month_Start: A flag for the first day of the month.\n",
    "* Year_Start: A flag for the first day of the year.\n",
    "* Week_Number: The week number of the year (1-52/53).\n",
    "\n",
    "Let's generate these using the  datetime functionality built into Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa67889-f545-45e4-9baf-a03bf586d917",
   "metadata": {},
   "source": [
    "## Code Block 2: Creating Time-Based Features\n",
    "\n",
    "First, we ensure the 'Time' column is in the correct datetime format. Then, we use the .dt accessor to pull out all the information we need. We use np.where to create our flags: if a condition is true (e.g., the day is a Monday), we assign it a value of 256; otherwise, it's 0. This high value creates a clear signal for our sonification tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c78ee-c2fd-4246-b70e-17092387e7fa",
   "metadata": {},
   "source": [
    "# Convert the 'Time' column to a proper datetime format\n",
    "df['Time'] = pd.to_datetime(df['Time'], format='mixed')\n",
    "\n",
    "# Extract time-based features that we can use for musical cues\n",
    "df['Week_Start'] = np.where(df['Time'].dt.day_name() == 'Monday', 256, 0)\n",
    "df['Month_Start'] = np.where(df['Time'].dt.is_month_start, 256, 0)\n",
    "df['Year_Start'] = np.where(df['Time'].dt.is_year_start, 256, 0)\n",
    "\n",
    "# We'll also extract the week number and year for grouping\n",
    "df['Week_Number'] = df['Time'].dt.isocalendar().week\n",
    "df['Year'] = df['Time'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9981c121-c2f0-4c6d-92a3-9de03d422761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Time' column to a proper datetime format\n",
    "df['Time'] = pd.to_datetime(df['Time'], format='mixed')\n",
    "\n",
    "# Create flag columns. A high value (256) is used to create a clear signal for our sonification tool.\n",
    "df['Week_Start'] = np.where(df['Time'].dt.day_name() == 'Monday', 256, 0)\n",
    "df['Month_Start'] = np.where(df['Time'].dt.is_month_start, 256, 0)\n",
    "df['Year_Start'] = np.where(df['Time'].dt.is_year_start, 256, 0)\n",
    "#df['Year_Quarter_Week'] = np.where(df['Time'].dt.isocalendar().week.isin([13, 26, 39, 52]), 256, 0) # <-- ADD THIS LINE BACK IN\n",
    "\n",
    "# We'll also extract the week number and year for grouping\n",
    "df['Week_Number'] = df['Time'].dt.isocalendar().week\n",
    "df['Year'] = df['Time'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce08c58-24f5-4ab0-941b-312e7b6a53de",
   "metadata": {},
   "source": [
    "# 5. From Daily Noise to a Weekly Melody\n",
    "Daily data can be noisy. To create a smoother, more melodic output, I decided to aggregate the data by week, taking the average (mean) traffic for each week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c8cb6-38b6-4936-8194-d02c9a3e7a23",
   "metadata": {},
   "source": [
    "## Code Block 3: Grouping by Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1907d07-b324-4ac7-b8f1-a00a031715a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the daily data by year and week number to create our weekly dataset\n",
    "weekly_df = df.groupby(['Year', 'Week_Number']).agg(\n",
    "    in_mean=('in', 'mean'),\n",
    "    out_mean=('out', 'mean'),\n",
    "    Year_Start=('Year_Start', 'max'),\n",
    "    Month_Start=('Month_Start', 'max'),\n",
    "    #Year_Quarter_Week=('Year_Quarter_Wee\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa56788-899d-46c9-86be-5419fda9e275",
   "metadata": {},
   "source": [
    "# 6. The Core Challenge: Proportional Sonification\n",
    "\n",
    "A key challenge is that download traffic is much larger than upload traffic. To prevent both from being mapped to the same highest musical note, we must scale them proportionally. This is done by calculating a scaling factor based on the maximum values in the original daily data, then applying it to the quantized upload data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d6f455-b412-4fb8-9b08-1687153968ef",
   "metadata": {},
   "source": [
    "## Code Block 4: Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b508fa5-b9aa-44a0-adc8-dd8c0414ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the scaling factor based on the max values in the original daily 'df'\n",
    "in_max = df['in'].max()\n",
    "out_max = df['out'].max()\n",
    "scaling_factor = out_max / in_max\n",
    "\n",
    "# Create the quantized 'in' column (download) with 48 bins for musical notes\n",
    "#weekly_df['in_quant'] = pd.cut(\n",
    "#    weekly_df['in_mean'], bins=48, labels=range(1, 49)\n",
    "#).astype(int)\n",
    "\n",
    "# Create the scaled, quantized 'out' column (upload)\n",
    "out_quant_scaled = (pd.cut(\n",
    "    weekly_df['out_mean'], bins=48, labels=range(1, 49)\n",
    ").astype(float) * scaling_factor)\n",
    "\n",
    "# Round the result and ensure the minimum note value is 1 (0 would be silence)\n",
    "#weekly_df['out_quant'] = out_quant_scaled.round().astype(int).clip(lower=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad46726-5f79-4a81-ac64-6612ece018bd",
   "metadata": {},
   "source": [
    "# 7. Adding Dynamics: Isolating Weekly Trends\n",
    "\n",
    "To give the final music a sense of movement, we need to isolate the weeks where traffic was increasing from the weeks where it was decreasing. The script achieves this by determining the trend from the **raw weekly average traffic** and using that to create two full sets of trend columns: one containing the raw values and one containing the final quantized \"musical notes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f1b65-fd8f-41d1-83d7-a27f0986a4b7",
   "metadata": {},
   "source": [
    "## Code Block 5: Capturing Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5babb4e-8916-4d5b-948c-0117715e7770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_90e71\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_90e71_level0_col0\" class=\"col_heading level0 col0\" >Year</th>\n",
       "      <th id=\"T_90e71_level0_col1\" class=\"col_heading level0 col1\" >Week_Number</th>\n",
       "      <th id=\"T_90e71_level0_col2\" class=\"col_heading level0 col2\" >in_mean</th>\n",
       "      <th id=\"T_90e71_level0_col3\" class=\"col_heading level0 col3\" >out_mean</th>\n",
       "      <th id=\"T_90e71_level0_col4\" class=\"col_heading level0 col4\" >Year_Start</th>\n",
       "      <th id=\"T_90e71_level0_col5\" class=\"col_heading level0 col5\" >Month_Start</th>\n",
       "      <th id=\"T_90e71_level0_col6\" class=\"col_heading level0 col6\" >in_up</th>\n",
       "      <th id=\"T_90e71_level0_col7\" class=\"col_heading level0 col7\" >in_down</th>\n",
       "      <th id=\"T_90e71_level0_col8\" class=\"col_heading level0 col8\" >out_up</th>\n",
       "      <th id=\"T_90e71_level0_col9\" class=\"col_heading level0 col9\" >out_down</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_90e71_row0_col0\" class=\"data row0 col0\" >2019</td>\n",
       "      <td id=\"T_90e71_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "      <td id=\"T_90e71_row0_col2\" class=\"data row0 col2\" >55040087.000000</td>\n",
       "      <td id=\"T_90e71_row0_col3\" class=\"data row0 col3\" >544081067.000000</td>\n",
       "      <td id=\"T_90e71_row0_col4\" class=\"data row0 col4\" >256</td>\n",
       "      <td id=\"T_90e71_row0_col5\" class=\"data row0 col5\" >256</td>\n",
       "      <td id=\"T_90e71_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "      <td id=\"T_90e71_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "      <td id=\"T_90e71_row0_col8\" class=\"data row0 col8\" >0.000000</td>\n",
       "      <td id=\"T_90e71_row0_col9\" class=\"data row0 col9\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e71_row1_col0\" class=\"data row1 col0\" >2019</td>\n",
       "      <td id=\"T_90e71_row1_col1\" class=\"data row1 col1\" >2</td>\n",
       "      <td id=\"T_90e71_row1_col2\" class=\"data row1 col2\" >128355950.857143</td>\n",
       "      <td id=\"T_90e71_row1_col3\" class=\"data row1 col3\" >1548699540.571429</td>\n",
       "      <td id=\"T_90e71_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_90e71_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "      <td id=\"T_90e71_row1_col6\" class=\"data row1 col6\" >128355950.857143</td>\n",
       "      <td id=\"T_90e71_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n",
       "      <td id=\"T_90e71_row1_col8\" class=\"data row1 col8\" >1548699540.571429</td>\n",
       "      <td id=\"T_90e71_row1_col9\" class=\"data row1 col9\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e71_row2_col0\" class=\"data row2 col0\" >2019</td>\n",
       "      <td id=\"T_90e71_row2_col1\" class=\"data row2 col1\" >3</td>\n",
       "      <td id=\"T_90e71_row2_col2\" class=\"data row2 col2\" >134956550.857143</td>\n",
       "      <td id=\"T_90e71_row2_col3\" class=\"data row2 col3\" >1562482549.714286</td>\n",
       "      <td id=\"T_90e71_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "      <td id=\"T_90e71_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "      <td id=\"T_90e71_row2_col6\" class=\"data row2 col6\" >134956550.857143</td>\n",
       "      <td id=\"T_90e71_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n",
       "      <td id=\"T_90e71_row2_col8\" class=\"data row2 col8\" >1562482549.714286</td>\n",
       "      <td id=\"T_90e71_row2_col9\" class=\"data row2 col9\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e71_row3_col0\" class=\"data row3 col0\" >2019</td>\n",
       "      <td id=\"T_90e71_row3_col1\" class=\"data row3 col1\" >4</td>\n",
       "      <td id=\"T_90e71_row3_col2\" class=\"data row3 col2\" >154850965.714286</td>\n",
       "      <td id=\"T_90e71_row3_col3\" class=\"data row3 col3\" >1763358201.142857</td>\n",
       "      <td id=\"T_90e71_row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "      <td id=\"T_90e71_row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "      <td id=\"T_90e71_row3_col6\" class=\"data row3 col6\" >154850965.714286</td>\n",
       "      <td id=\"T_90e71_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n",
       "      <td id=\"T_90e71_row3_col8\" class=\"data row3 col8\" >1763358201.142857</td>\n",
       "      <td id=\"T_90e71_row3_col9\" class=\"data row3 col9\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_90e71_row4_col0\" class=\"data row4 col0\" >2019</td>\n",
       "      <td id=\"T_90e71_row4_col1\" class=\"data row4 col1\" >5</td>\n",
       "      <td id=\"T_90e71_row4_col2\" class=\"data row4 col2\" >170757284.571429</td>\n",
       "      <td id=\"T_90e71_row4_col3\" class=\"data row4 col3\" >1828013320.000000</td>\n",
       "      <td id=\"T_90e71_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "      <td id=\"T_90e71_row4_col5\" class=\"data row4 col5\" >256</td>\n",
       "      <td id=\"T_90e71_row4_col6\" class=\"data row4 col6\" >170757284.571429</td>\n",
       "      <td id=\"T_90e71_row4_col7\" class=\"data row4 col7\" >0.000000</td>\n",
       "      <td id=\"T_90e71_row4_col8\" class=\"data row4 col8\" >1828013320.000000</td>\n",
       "      <td id=\"T_90e71_row4_col9\" class=\"data row4 col9\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e367bbf490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. First, define the trend direction based on the raw weekly mean values\n",
    "is_in_up = weekly_df['in_mean'] > weekly_df['in_mean'].shift()\n",
    "is_in_down = weekly_df['in_mean'] < weekly_df['in_mean'].shift()\n",
    "is_out_up = weekly_df['out_mean'] > weekly_df['out_mean'].shift()\n",
    "is_out_down = weekly_df['out_mean'] < weekly_df['out_mean'].shift()\n",
    "\n",
    "# 2. Create trend columns containing the RAW mean values\n",
    "weekly_df['in_up'] = weekly_df['in_mean'].where(is_in_up, 0)\n",
    "weekly_df['in_down'] = weekly_df['in_mean'].where(is_in_down, 0)\n",
    "weekly_df['out_up'] = weekly_df['out_mean'].where(is_out_up, 0)\n",
    "weekly_df['out_down'] = weekly_df['out_mean'].where(is_out_down, 0)\n",
    "\n",
    "display(weekly_df.head().style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7257f5a-1464-460d-aeb2-6c2d597a68d2",
   "metadata": {},
   "source": [
    "# 8. Final Polish and Saving\n",
    "\n",
    "Finally, we perform some housekeeping. The mean columns are renamed (e.g., `in_mean` becomes `in`) and the columns are selected and reordered to exactly match the desired final format before saving the result to a CSV file in the `/output` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c6c815-ecbf-448f-be2c-d73f82577bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing the DataFrame...\n",
      "Processing complete. File saved to: ./output/Processed_traffic2.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7f049\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_7f049_level0_col0\" class=\"col_heading level0 col0\" >in</th>\n",
       "      <th id=\"T_7f049_level0_col1\" class=\"col_heading level0 col1\" >out</th>\n",
       "      <th id=\"T_7f049_level0_col2\" class=\"col_heading level0 col2\" >Month_Start</th>\n",
       "      <th id=\"T_7f049_level0_col3\" class=\"col_heading level0 col3\" >Year_Start</th>\n",
       "      <th id=\"T_7f049_level0_col4\" class=\"col_heading level0 col4\" >in_up</th>\n",
       "      <th id=\"T_7f049_level0_col5\" class=\"col_heading level0 col5\" >in_down</th>\n",
       "      <th id=\"T_7f049_level0_col6\" class=\"col_heading level0 col6\" >out_up</th>\n",
       "      <th id=\"T_7f049_level0_col7\" class=\"col_heading level0 col7\" >out_down</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_7f049_row0_col0\" class=\"data row0 col0\" >55040087.000000</td>\n",
       "      <td id=\"T_7f049_row0_col1\" class=\"data row0 col1\" >544081067.000000</td>\n",
       "      <td id=\"T_7f049_row0_col2\" class=\"data row0 col2\" >256</td>\n",
       "      <td id=\"T_7f049_row0_col3\" class=\"data row0 col3\" >256</td>\n",
       "      <td id=\"T_7f049_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "      <td id=\"T_7f049_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "      <td id=\"T_7f049_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
       "      <td id=\"T_7f049_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7f049_row1_col0\" class=\"data row1 col0\" >128355950.857143</td>\n",
       "      <td id=\"T_7f049_row1_col1\" class=\"data row1 col1\" >1548699540.571429</td>\n",
       "      <td id=\"T_7f049_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_7f049_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_7f049_row1_col4\" class=\"data row1 col4\" >128355950.857143</td>\n",
       "      <td id=\"T_7f049_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
       "      <td id=\"T_7f049_row1_col6\" class=\"data row1 col6\" >1548699540.571429</td>\n",
       "      <td id=\"T_7f049_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7f049_row2_col0\" class=\"data row2 col0\" >134956550.857143</td>\n",
       "      <td id=\"T_7f049_row2_col1\" class=\"data row2 col1\" >1562482549.714286</td>\n",
       "      <td id=\"T_7f049_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_7f049_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_7f049_row2_col4\" class=\"data row2 col4\" >134956550.857143</td>\n",
       "      <td id=\"T_7f049_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
       "      <td id=\"T_7f049_row2_col6\" class=\"data row2 col6\" >1562482549.714286</td>\n",
       "      <td id=\"T_7f049_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7f049_row3_col0\" class=\"data row3 col0\" >154850965.714286</td>\n",
       "      <td id=\"T_7f049_row3_col1\" class=\"data row3 col1\" >1763358201.142857</td>\n",
       "      <td id=\"T_7f049_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_7f049_row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "      <td id=\"T_7f049_row3_col4\" class=\"data row3 col4\" >154850965.714286</td>\n",
       "      <td id=\"T_7f049_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "      <td id=\"T_7f049_row3_col6\" class=\"data row3 col6\" >1763358201.142857</td>\n",
       "      <td id=\"T_7f049_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_7f049_row4_col0\" class=\"data row4 col0\" >170757284.571429</td>\n",
       "      <td id=\"T_7f049_row4_col1\" class=\"data row4 col1\" >1828013320.000000</td>\n",
       "      <td id=\"T_7f049_row4_col2\" class=\"data row4 col2\" >256</td>\n",
       "      <td id=\"T_7f049_row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "      <td id=\"T_7f049_row4_col4\" class=\"data row4 col4\" >170757284.571429</td>\n",
       "      <td id=\"T_7f049_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
       "      <td id=\"T_7f049_row4_col6\" class=\"data row4 col6\" >1828013320.000000</td>\n",
       "      <td id=\"T_7f049_row4_col7\" class=\"data row4 col7\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e366e66a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Finalizing the DataFrame...\")\n",
    "\n",
    "# Rename the '_mean' columns for a cleaner final output\n",
    "final_df = weekly_df.rename(columns={\n",
    "    'in_mean': 'in',\n",
    "    'out_mean': 'out'\n",
    "})\n",
    "\n",
    "# Define exactly which columns to keep in the final CSV file.\n",
    "# This list also sets the order of the columns.\n",
    "columns_to_keep = [\n",
    "    'in', 'out',\n",
    "    'Month_Start', 'Year_Start',\n",
    "    'in_up', 'in_down',\n",
    "    'out_up', 'out_down'\n",
    "]\n",
    "\n",
    "# Create the final DataFrame with only the selected columns in the correct order\n",
    "final_df = final_df[columns_to_keep]\n",
    "\n",
    "# Define the output path and filename\n",
    "output_filename = \"./output/Processed_traffic.csv\"\n",
    "\n",
    "# Save the final DataFrame to a CSV file.\n",
    "# We use index=False to prevent writing the row numbers (0, 1, 2...) to the file.\n",
    "final_df.to_csv(output_filename, index=False)\n",
    "\n",
    "# Print a confirmation message that uses the correct filename\n",
    "print(f\"Processing complete. File saved to: {output_filename}\")\n",
    "\n",
    "# Display the first few rows of the final data to confirm it's correct\n",
    "display(final_df.head().style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ffed2-7aa6-4fc5-9cee-c4a5f394ea49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# 8. The Final Composition: Making Music with TwoTone  \n",
    "\n",
    "With our data prepared, the final step is to bring it to life. I used the [TwoTone MIDI Out Beta](https://twotone-midiout-beta.netlify.app/) . You simply upload the final CSV, map your data columns to musical parameters, and press play.  \n",
    "Here are the settings I found worked best to create a clear and pleasant composition from the weekly data:\n",
    "\n",
    "**Download Data (in_quant)**\n",
    "\n",
    "* Instrument: Double Bass\n",
    "* Range: 2 Octaves\n",
    "* Speed: 4x\n",
    "* Style: Arpeggio (4 notes, ascending)\n",
    "\n",
    "**Upload Data (out_quant)**\n",
    "\n",
    "* Instrument: Glockenspiel\n",
    "* Range: 2 Octaves\n",
    "* Speed: 2x\n",
    "* Style: Ascending\n",
    "\n",
    "The low, heavy notes of the double bass represent the high volume of download traffic, while the lighter, twinkling glockenspiel represents the lower volume of upload traffic. The time markers for month and year starts were used as cues to add speech annotations (e.g., \"2024,\" \"February\") using an external audio editor for the final video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7011db1-420a-47e4-b011-d67fe3025d14",
   "metadata": {},
   "source": [
    "# 9. Conclusion\n",
    "This project was a fascinating journey into a new, for me, form of data representation. It demonstrates that with a bit of creativity and some data manipulation skills, we can find stories in data that go beyond traditional charts. Sonification offers a uniquely emotional and intuitive way to experience data patterns over time.\n",
    "\n",
    "The final CSV file is ready for sonification. I encourage you to download the notebook from my GitHub [link to your GitHub repo here], experiment with the data, and create your own data-driven music with the [TwoTone MIDI Out Beta](https://twotone-midiout-beta.netlify.app/)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
